---
title: "Processing and Analysis of Biological Data"
subtitle: "Chapter 5. Generalized Linear Models (GLM)"
author: "Ã˜ystein H. Opedal, Department of Biology, Lund University"
date: "`r Sys.Date()`"
output: pdf_document
fig_caption: yes
pandoc_args: ["--wrap=none"]
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

Generalized linear models (GLMs) add flexibility to the linear model by allowing deviations from the usual assumption of normally distributed residuals. Briefly, a GLM consists of the familiar linear predictor of a linear model (often denoted as $\eta$),

$\eta = \beta_0 + \Sigma_j x_{ij} \beta_j + \epsilon_i$

and a link function, $g$, that places the predictor on a Gaussian (normally-distributed) scale.

$$y = g^{-1}(\eta)$$
Before going into details about GLMs, we need to recall some basics about the most common error distributions used in biological data analyses.

### The Binomial distribution

The binomial distribution has two parameters, *n* and *p*, and summarizes a set of so-called Bernoulli trials with two possible outcomes, yes (1) or no (0). When we have performed more than one trial, we can compute the proportion *p* of the *n* trials with a positive outcome.

The theoretical variance of the binomial distribution is given by 

$\sigma^2 = np(1-p)$

```{r, fig.width=4, fig.height=4}
x = seq(from=0, to=1, by=0.01)
v_b = x*(1-x) #Binomial variance
plot(x, v_b, type="l", xlab="Probability", ylab="Theoretical variance", las=1)
```

This is important to keep in mind, because it affects how we can compare the (proportional) variation of variables measures as proportions. If e.g. one population has a mean of 0.5, if will be expected to be much more variable than a second population with a mean of 0.1, just because there is less opportunity to vary. This is a now well-recognized problem e.g. in demography research, where the interest is often in comparing the extent of variation in life-history traits measured as proportions, such as germination or survival. One proposed solution is to scale the observed CV by the maximum based on the theoretical variance, but a perhaps simpler approach is to transform the proportional data in a way that makes them approach a normal distribution.

One previously popular but now not recommended transformation is the so-called arcsine square-root transformation,

$x' = \arcsin(\sqrt{x})$

A more meaningful transformation is the logit or log odds transformation

$logit(x) = log(\frac{x}{1-x})$

In the following example we are using some functions. This may be a good time to visit the Appendix introducing some basics on writing functions. Note that when functions are given on a single line, we can skip the special curvy brackets (`{`).

```{r, fig.width=6, fig.height=6}
logit = function(x) log(x/(1-x))
invlogit = function(x) 1/(1+exp(-x))

x = runif(200)
logit_x = logit(x)

par(mfrow=c(2,2))
hist(x, las=1)
hist(logit_x, las=1)

xx = seq(-5, 5, 0.01)
plot(xx, invlogit(xx), type="l", las=1,
     xlab="Logit (x)",
     ylab="P")
plot(x, invlogit(logit_x), las=1)
```

The logit transformation is the most common *link function* in a Generalized Linear Model with binomial errors. A similar alternative is the so-called probit link, which corresponds to the quantile distribution of the standard normal distribution (which is why we can use the `pnorm` function to compute the inverse).

```{r, fig.height=4, fig.width=4}
plot(xx, invlogit(xx), type="l", las=1,
     xlab="Logit/Probit (x)",
     ylab="P")
lines(xx, pnorm(xx), lty=2)
legend("topleft", legend=c("Logit", "Probit"),
       lty=c(1,2), bty="n")
```

## Logistic regression
As we have already seen, a logistic regression (GLM with binomial errors) is well suited for analysing binary data (or proportions).

Below, we simulate data by first formulating a linear predictor $\eta$, then transforming the predicted values into probabilities (through the inverse logit transformation), and finally binarizing the data by sampling from the binomial distribution. The last step adds additional uncertainty by accounting for the stochasticity of the observation process.

```{r, fig.height=3.5, fig.width=10}
x = rnorm(200, 10, 3)
eta = -2 + 0.4*x + rnorm(200, 0, 2)
p = invlogit(eta)
y = rbinom(200, 1, p)

par(mfrow=c(1,3))
plot(x, eta, las=1)
plot(x, p, las=1)
plot(x, y, las=1)
```

To fit the generalized model, we use the `glm` function and specify the error family (binomial) and link function (here *logit*).

```{r}
m = glm(y~x, family=binomial(link="logit")) 
summary(m)
```

The summary table includes, as always, a lot of information. The first thing to be aware is that when we are fitting a GLM, we obtain the parameter estimates on the link scale (here logit). Note that the parameter estimates are not too far from those we used to define the linear predictor $\eta$ when we simulated the data. These values are meaningful as such, and if the predictor variable has units of *mm*, the slopes have units of *log odds* $mm^{-1}$.

To interpret the results biologically and to represent them in graphs, it can be useful to backtransform the predicted values to the probability scale. For example, we can ask how much the probability changes for a standard deviation increase in the predictor variable (though note that this is no longer a linear transform, so the consequences of increasing and decreasing the predictor by one standard deviation may be different).

Recall from the previous section that a log odds of 0 corresponds to a probability of 0.5  ($log(\frac{0.5}{1-0.5})=log(1)=0$). If we solve the model equation (the linear predictor) for 0, we can thus obtain the predictor value corresponding to a probability of 0.5, which is often a relevant benchmark.

$0 = \beta_0 + \beta_xx$

$\frac{-\beta_0}{\beta_x}=x$

EXERCISE: Replicate the plot below. To produce a regression line, we define some new $x$-values that spans the data range along the $y$-axis, then obtain predicted values $\hat{y}$ (using the model coefficients), and finally transform these values to the probability scale to obtain the predicted probabilities $\hat{p}$.

```{r}
coefs = summary(m)$coef

x_pred = seq(from=min(x), to=max(x), by=0.01)
y_hat = coefs[1,1] + coefs[2,1]*x_pred
p_hat = invlogit(y_hat)
```

Compute the value of $x$ corresponding to a probability of 0.5, and add lines to the plot to illustrate the results.

```{r, fig.height=4, fig.width=4, echo=F}
coefs = summary(m)$coef

x_pred = seq(from=min(x), to=max(x), by=0.01)
y_hat = coefs[1,1] + coefs[2,1]*x_pred
p_hat = invlogit(y_hat)

plot(x, y, las=1)
lines(x_pred, p_hat)
abline(h=0.5, lty=2)
abline(v=-coefs[1,1]/coefs[2,1], lty=2)
```

The GLM summary table does not provide an $r^2$ value, because the normal $r^2$ does not work for logistic regression. There are however several 'Pseudo-$r^2$' available, typically based on comparing the likelihood of the model to that of a null model (a similar model but with only an intercept). The `MuMIn` package provides one such measure.

```{r, warning=F}
library(MuMIn)
r.squaredGLMM(m)
```

Another possibility for logistic regression is to compute the coefficient of discrimination, or Tjur's $D$. Indeed, there are several ways to evaluate the performance of a statistical model, and in a logistic regression it makes sense to ask how well the model discriminates between true positives and negatives in the data.

The coefficent of discrimination is defined as $D=\bar{\hat{\pi_1}}-\bar{\hat{\pi_0}}$, and is computed by comparing the predicted probability for those data points that are successes or positives (1s; $\bar{\hat{\pi_1}}$) to the predicted probability for those data points that are failures or negatives (0s; $\bar{\hat{\pi_1}}$).

```{r, fig.height=4, fig.width=5, echo=F}
xx = seq(0,9, 0.01)
yvals = invlogit(-3+0.7*xx)
plot(xx, yvals, type="l", ylim=c(0,1), las=1, 
     xlim=c(0, 12.5), bty="l",
     ylab="Probability", xlab="", xaxt="n")
mtext("Predictor", 1, 1)
xvals = c(1,2,3,6,7,8)
yvals = c(0,0,0,1,1,1)
points(xvals, yvals, pch=16)
segments(xvals, yvals, xvals, invlogit(-3+0.7*xvals))

segments(1, mean(invlogit(-3+0.7*xvals)[1:3]),9.5, mean(invlogit(-3+0.7*xvals)[1:3]), lty=2)
segments(6, mean(invlogit(-3+0.7*xvals)[4:6]),9.5, mean(invlogit(-3+0.7*xvals)[4:6]), lty=2)

xpos = 10
text(xpos, mean(invlogit(-3+0.7*xvals)[1:3]), expression(paste(bar(hat(pi[0])))), adj=0)
text(xpos, mean(invlogit(-3+0.7*xvals)[4:6]), expression(paste(bar(hat(pi[1])))), adj=0)

text(xpos, mean(invlogit(-3+0.7*xvals)[1:6]), adj=0,
     expression(paste("D = ", bar(hat(pi[1]))-bar(hat(pi[0])))))

arrows(9.75, mean(invlogit(-3+0.7*xvals)[1:3]), 9.75, mean(invlogit(-3+0.7*xvals)[4:6]),
       code=3, angle=90, length=0.05, lty=1)
```


```{r}
y_hat = coefs[1,1] + coefs[2,1]*x
p_hat = invlogit(y_hat)

mean(p_hat[which(y==1)]) - mean(p_hat[which(y==0)])
```

Some final notes on fitting binomial GLM's. There are three ways to formulate these models in R. In the example above, the data were 0's and 1's, and we could specify the model simply as 

`glm(y ~ x, family=binomial(link="logit"))`

When each observation is based on more than one trial, we can formulate the model in two ways. The first is 

`glm(y ~ x, family=binomial(link="logit"), weights=n)` 

where `y` is the proportion of successes, and `n` is the number of trials. The second method is to fit a two-column matrix as response variable, where the first colomn is the number of successes, and the second column is the number of failures, i.e. `y = cbind(successes, failures)`.  The model formula is then

`glm(cbind(successes, failures) ~ x, family=binomial(link="logit"))` 

## Data exercise: seed germination
The following data are from a study investigating patterns of seed dormancy in a plant. In this plant species, seeds needs to stay in dry conditions for a specific amount of time before they are ready to germinate, a process known as 'after-ripening'. Once the seeds are ripe they will normally germinate when exposed to favourable (moist) conditions.

After different durations of after-ripening, the seeds were sown on moist soil and their germination success (proportion of seeds germinated) recorded. The seeds came from four different populations, and were weighed (in *mg*) prior to sowing.

The variables in the data are as follows:

- `pop` = Population ID
- `mother` = Maternal plant ID
- `crossID` = Unique cross identifier
- `blocktray` = Sowing tray (experimental block)
- `timetosowing` = Time in days from seed dispersal to watering
- `MCseed` = Population-mean-centered seed mass in mg
- `nseed` = Number of seeds sown
- `germ2` = Proportion of seeds germinated

Analyse the data to estimate the pattern of germination success in response to variation in the duration of after-ripening. Are the patterns similar in different populations? Are there other factors affecting germination success? Produce relevant summary statistics, parameter estimates, and graphs. 

```{r}
dat = read.csv("datasets/dormancy/dormancy.csv")
names(dat)
```

As a suggested start, the following lines fit a simple model to data from one population using two different methods. Note that the model fit is the same (the log Likelihood of the two models is identical).

```{r}
subdat = dat[dat$pop=="CC",]

germ = subdat$germ2 * subdat$nseed #Successes
notgerm = subdat$nseed - germ #Failures

mod1 = glm(cbind(germ, notgerm) ~ timetosowing, "binomial", data=subdat)
mod2 = glm(germ2 ~ timetosowing, "binomial", weights=nseed, data=subdat)
logLik(mod1) == logLik(mod2)
```

Can you use the fitted models to estimate the duration of after-ripening required for the expected germination rate to be 0.5?


## Poisson and negative-binomial regression

A second very common data type in biology is count data, which occurs when we have counted something. In ecological studies we often count individuals or species, and in evolutionary biology we often count e.g. the number of offspring. For such data, the data distribution is often skewed, and the variance tends to increase with the mean. The Poisson distribution is tailored for such data.

```{r, fig.height=4, fig.width=4}
x = rpois(200, 3)
hist(x, las=1)
```

The Poisson distribution has a single parameter $\lambda$ that determines both the mean and the variance, so that $E(X) = Var(X) = \lambda$. Thus, the variance increases linearly with the mean. The probability mass function changes quite dramatically for different values of $\lambda$. For low values the distribution is highly skewed, for high values the distribution approaches a Gaussian (normal) distribution. However, the variance is still constrained to be the same as the mean, which is not the case for the normal distribution.

```{r, fig.height=4, fig.width=4}
x = seq(0, 20, 1)
y = dpois(x, lambda=1)
plot(x,y, type="b", las=1, xlab="k", ylab="P(x=k)", pch=16, col=1)
points(x, dpois(x, lambda=3), type="b", pch=16, col=2)
points(x, dpois(x, lambda=10), type="b", pch=16, col=3)
legend("topright", col=1:3, pch=16, 
       legend=c(expression(paste(lambda, " = 1")),
                expression(paste(lambda, " = 3")),
                expression(paste(lambda, " = 10"))))
```

The distribution of count data can sometimes by normalized through a log-transformation, and the log is indeed the link function of a Poisson regression model. The alternative method of log-transforming the data and then fitting a Gaussian model is problematic when there are zeros in the data. Adding a constant (e.g. 0.5 or 1) is sometimes an option, but is generally not recommended. A better option is to analyze the data in a GLM framework with Poisson-distributed errors and a log link function.

```{r}
x = rnorm(200, 10, 3)
eta = -2 + 0.2*x
y = ceiling(exp(eta + rpois(200, 0.3)))

par(mfrow=c(1,2))
plot(x, eta, las=1)
plot(x, y, las=1)
```

```{r}
m = glm(y~x, family="poisson")
summary(m)
```

When interpreting Poisson regressions, there are several things to keep in mind. First, as in all GLMs, the parameters are reported on the link scale, here log. Recall that the link scale has useful proportional properties, so that the slope can be interpreted roughly as the proportional change in y per unit change in x. To plot the fitted regression line, we have to back-transform the predicted values.  This time we use the generic `predict` function to obtain the predicted values on the data scale, and to construct a 95% confidence polygon.

```{r, fig.height=4, fig.width=4}
plot(x, y, las=1, col="darkgrey", pch=16)
xx = seq(min(x), max(x), 0.01)
y_hat = predict(m, newdata=list(x=xx), type="response", se.fit=T)
lines(xx, y_hat$fit)
#lines(xx, y_hat$fit+1.96*y_hat$se.fit, lty=2)
#lines(xx, y_hat$fit-1.96*y_hat$se.fit, lty=2)

polygon(c(xx, rev(xx)), 
        c(y_hat$fit+1.96*y_hat$se.fit, 
          rev(y_hat$fit-1.96*y_hat$se.fit)), 
        col = rgb(0,1,0,.5), border = FALSE)
```

As in logistic regression the normal $r^2$ is not valid, and we can use e.g. the `r.squaredGLMM` function to obtain a Pseudo $r^2$. For a simple GLM an older Pseudo $r^2$ is $$1-\frac{Residual ~ deviance}{Null ~ deviance}$$.

```{r}
r.squaredGLMM(m)
1-(m$deviance/m$null.deviance)
```

The *deviance* is a measure of how far our model is from a "perfect" or saturated model, i.e. one that perfectly explains the data. Fitting a GLM is done by maximizing the likelihood of a model, which is the same as minimizing the deviance. The deviance is defined mathematically as 2 times the difference in the log likelihood of the model and a saturated model, and can be used to compare alternative models. For example, the Pseudo $r^2$ above is based on comparing the (mis)fit of the focal model to a null model with only an intercept. If a model is as bad as a null model, the Pseudo $r^2$ will be 0. We can also use the deviance to compare other models, e.g. through likelihood ratio tests (more on that later).

Second, recall that the Poisson distribution has a single parameter $\lambda$ determining both the mean and the variance. In real count data the variance often increase disproportionally compared to the mean, a phenomenon called *overdispersion*. Biologically, this occurs because we tend to observe multiple entities together. For example, in a survey of common eiders, we will often see either no eiders, or a lot of eiders.

We can quantify overdispersion in the data based on the fitted model by calculating the ratio of the residual deviance to the residual degrees of freedom. In the model above there is no serious overdispersion, because the residual deviance is only a little larger than the residual degrees of freedom. Let us construct an example with more serious overdispersion.


```{r}
set.seed(1)
x = rnorm(200, 10, 3)
eta = -2 + 0.2*x
y = floor(exp(eta + rnbinom(200, 1, mu=.8)))

par(mfrow=c(1,2))
plot(x, eta, las=1)
plot(x, y, las=1)
```

```{r}
m = glm(y~x, family="poisson")
summary(m)
```

Here the overdispersion is serious, and we can not trust the model estimates. In this case, we need an alternative link function that allows the variance to increase more than the mean. The negative binomial distribution is a good option. The negative binomial is similar to the Poisson distribution, but includes an additional parameter modelling the disproportionate increase in variance with increasing mean.

```{r}
library(MASS)
m = glm.nb(y~x)
```

\newpage
```{r}
summary(m)
```

## Data exercise: bee distribution

The following dataset includes the abundance of the bee species *Eulaema nigrita* in the Brazilian Atlantic forest, and a number of potential predictor variables including climate (mean annual temperature and precipitation, temperature and precipitation seasonality) and land use (proportion forest cover and land use heterogeneity defined as the Shannon diversity ($-\Sigma_i p_i ln p_i$) of local land-use classes).

The climate data from the *Worldclim* dataset are much used in ecological studies. The mean annual temperature (MAT) is given in degrees Celsius times 10, the annual precipitation in mm, the temperature seasonality (Tseason) as 100 times the standard deviation of the monthly temperature, and the precipitation seasonality (Pseason) as the CV of monthly precipitation (given as a percent, i.e. times 100). Why is the temperature seasonality measured as a standard deviation and the precipitation as a CV?

The *effort* variable in the log number of hours of sampling, and the *method* variable indicates whether the collecting was done with hand nets, traps, or a combination of both.

Use a GLM to build a model explaining the distribution patterns of *Eulaema nigrita*. Interpret the results and produce nice tables and figures.

```{r}
dat = read.csv("datasets/Eulaema.csv")
head(dat)
```

## Hurdle models

In biology we often have count data with many zeros. This is one of the causes of overdispersion as discussed above. Another way to deal with this is to split the analysis into two independent components, where the first models the zeros, and the second models the counts given that at least 1 entity was observed. This is called a hurdle model, where hurdle refers to the separation of zeros from non-zeros. We can fit a hurdle model in two parts and then combine the predictions.

Below we define two new response variables, where the first ($y1$) is a 0/1 variable, and the second ($y2$) has all the zeros set to `NA`.

```{r}
y1 = ((y>1)*1)
m1 = glm(y1~x, family="binomial" (link="logit"))

y2 = y
y2[which(y==0)] = NA

m2 = glm(y2~x, family="poisson", na=na.exclude)
```

```{r, fig.width=7, fig.height=3}
coefs1 = summary(m1)$coef
coefs2 = summary(m2)$coef
y_hat1 = coefs1[1,1] + coefs1[2,1]*x
y_hat2 = coefs2[1,1] + coefs2[2,1]*x

y_pred = invlogit(y_hat1)*exp(y_hat2)

par(mfrow=c(1,3))
plot(x, invlogit(y_hat1), las=1)
plot(x, exp(y_hat2), las=1)
plot(x, y_pred, las=1)
```


## Data exercise: bee distribution

The following dataset includes the abundance of the bee species *Eulaema nigrita* in the Brazilian Atlantic forest, and a number of potential predictor variables including climate (mean annual temperature and precipitation, temperature and precipitation seasonality) and land use (proportion forest cover and land use heterogeneity defined as the Shannon diversity of local land-use classes). Use a GLM to build a model explaining the distribution patterns of *Eulaema nigrita*. Interpret the results and produce nice tables and figures.

```{r}
dat = read.csv("datasets/Eulaema.csv")
head(dat)
```

