---
title: "Processing and Analysis of Biological Data"
subtitle: "Chapter 8. A primer of multivariate analysis"
author: "Ã˜ystein H. Opedal, Department of Biology, Lund University"
date: "`r Sys.Date()`"
output: pdf_document
fig_caption: yes
pandoc_args: ["--wrap=none"]
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

Biological data are very often multivariate, in the sense that we are dealing with potentially large sets of correlated variables. Some times the patterns of covariation among variables is the focus of the investigation, and other times we may have taken several measurements that describe similar aspects of the biology in question. In any case, we very often have to deal with analyses of multiple variables. Note that when we refer to multivariate analyses, we mean analyses where there is more than one response variable. Thus, a multiple regression is normally considered a univariate analysis even though there are multiple correlated predictor variables.

## Variance matrices and eigendecomposition

Multivariate data can be summarized as *variance matrices*, which are symmetrical matrices with variances on the diagonal and covariances on the off-diagonals. We will work with the following example variance matrix $\textbf{C}$.

```{r}
C = matrix(c(0.7, 0.2, -0.3,
              0.2, 1.2, 0.4,
              -0.3, 0.4, 0.6), 
              nrow=3)
C
```

EXERCISE: One way to confirm that a matrix is symmetrical is to show that it is identical to its transpose ($\textbf{A}=\textbf{A}^T$). Confirm that $\textbf{C}$ is symmetrical. To transpose a matrix in R we can use the `t` function.

```{r, echo=F}
C == t(C)
```

Correlations are standardized covariances, i.e. they are the covariances between standardized variables (mean = 0, sd = 1). We can compute the correlation between two variables as 

$Cor(x,y)=Cov(x,y)/\sqrt{Var(x)Var(y)}$.

EXERCISE: Translate the covariance matrix into a correlation matrix.

```{r, echo=F}
cormat = matrix(NA, nrow=3, ncol=3)
for(i in 1:3){
  for(j in 1:3){
    cormat[i,j] = C[i,j]/sqrt(C[i,i]*C[j,j])
  }}
round(cormat, 3)
```

### The multivariate normal distribution
We can use a variance matrix to simulate data from the multivariate normal distribution $MVN(\bar{x}, \Sigma)$, where $\Sigma$ is a variance matrix.

```{r, fig.height=4, fig.width=4, warning=F, message=F}
library(MASS)
library(ellipse)
set.seed(1)

X = data.frame(mvrnorm(200, mu=c(0,0,0), Sigma=C))
colnames(X) = c("z1", "z2", "z3")
head(X)
means = c(apply(X[,1:2], 2, mean))

plot(X$z1, X$z2, las=1)
lines(ellipse(cov(X[,1:2]), centre=means))
```

## Eigendecomposition

Variance matrices have several useful properties for multivariate analysis. A common operation is a so-called eigendecomposition (or spectral decomposition).

A vector $v$ is an eigenvector of the matrix $\textbf{A}$ if it satisfies the condition

$\textbf{Av}=\lambda\textbf{v}$,

where $\lambda$ is an eigenvalue of $\textbf{A}$. From this follows also the relation

$\textbf{A}=\textbf{Q} \Lambda \textbf{Q}^{-1}$.

where $\textbf{Q}$ is a matrix with the eigenvectors in columns, and $\Lambda$ is a square matrix with the eigenvalues on the diagonal.

Biologically, the eigen analysis allows us to 'rotate' the variation in the data (say, in the multivariate phenotype of an organism) so that the first 'trait' (leading eigenvector) represents the multivariate direction of most variation. Often, this can be interpreted roughly as the size of the organism. The subsequent eigenvectors represent other axes of variation, that could e.g. represent shape. The subsequent eigenvectors are *orthogonal*, so that e.g. the second eigenvector is perpendicular to the first.

In `R`, the function `eigen` performs the eigendecomposition and returns the eigenvectors and corresponding eigenvalues.

\newpage

```{r}
eigen(C)
```
The eigenvalues represent the amount of variance associated with each eigenvector (given in columns). We can thus compute the proportion of variance associated with each eigenvector as $\lambda_i/\sum\lambda$.

```{r, fig.height=4, fig.width=4}
plot(X$z1, X$z2, las=1, col="grey")
lines(ellipse(cov(X[,1:2]), centre=means))
arrows(means[1], means[2], 
       means[1]+eigen(C)$vectors[1,1], 
       means[2]+eigen(C)$vectors[2,1],
       code=2, length=0.1, lwd=2)
arrows(means[1], means[2], 
       means[1]+eigen(C)$vectors[1,2], 
       means[2]+eigen(C)$vectors[2,2],
       code=2, length=0.1, lwd=2)
```

Before continuing, we need to recall the rules for matrix multiplication. There are several forms of matrix multiplication, but the 'normal' matrix multiplication requires that the number of columns in the first matrix equals the number of rows in the second matrix, and the resulting matrix will have the same number of rows as the first matrix, and the same number of columns as the second matrix. If we multiply a matrix of dimensions $m \times n$ with one of dimensions $n \times l$, we get a matrix of dimensions $m \times l$. The matrix multiplication operator in `R` is `%*%`.

EXERCISE: Compute the proportion of variance associated with each eigenvector of $\textbf{C}$.

```{r, echo=F}
eigen(C)$values/sum(eigen(C)$values)
```

EXERCISE: Confirm that the eigenvectors are of unit length (length = 1) and that the angle between them is 90 degrees.

Recall that the length of a vector is the square root of the sum of the squared vector elements, and the angle between two unit-length vectors $u_1$ and $u_2$ is $\frac{180}{\pi}cos^{-1}(u_1u_2)$.

Length of the eigenvectors

```{r, echo=F}
apply(eigen(C)$vectors, 2, function(x) sqrt(sum(x^2)))
```

Angle between first and second eigenvector

```{r, echo=F}
180/pi * acos(t(eigen(C)$vectors[,1]) %*% eigen(C)$vectors[,2])
```


EXERCISE: Reconstruct the matrix $\textbf{C}$ from the eigenvalues and eigenvectors. Hint: To get the inverse of a matrix, we can use the `solve` function.   

```{r, echo=F}
C
eigen(C)$vectors %*% diag(eigen(C)$values) %*% solve(eigen(C)$vectors)
```

```{r, include=F}
eigen(C)$vectors[1:2, 1:2] %*% 
diag(eigen(C)$values)[1:2, 1:2] %*% 
solve(eigen(C)$vectors)[1:2, 1:2]
```


## Principal Component Analysis

Eigenanalysis is a core component of principal component analysis. In it's simplest form, the principal components are the same as the eigenvectors. Let us derive some new traits along the eigenvectors of $\textbf{C}$.

```{r}
dim(as.matrix(X))
dim(as.matrix(eigen(C)$vectors[,1]))

t1 = as.matrix(X) %*% eigen(C)$vectors[,1]
t2 = as.matrix(X) %*% eigen(C)$vectors[,2]
t3 = as.matrix(X) %*% eigen(C)$vectors[,3]

c(var(X[,1]), var(X[,2]), var(X[,3]))
c(var(t1), var(t2), var(t3))
```

Notice that the variances of new traits decreases from the first to the third trait, which was not the case for the original traits. However, the total variance stays the same (because we have just reorganized the variation).

```{r}
var(t1) + var(t2) + var(t3)
var(X[,1]) + var(X[,2]) + var(X[,3])
```

The eigenvectors are *orthogonal*, i.e. they are not correlated with each other.

A very similar operation is performed by several `R` packages for principal component analysis, e.g. `prcomp`. The principal components are not exactly the same as defining traits along the eigenvectors, but are subject to some further rotation. However, the principal components will be strongly correlated with the traits defined along the eigenvectors.

```{r}
pca = princomp(X)
summary(pca)
```

The proportion of variance explained by each principal component is computed as the variance of each principal component divided by the total, which is basically equal to the corresponding eigenvalue divided by the sum of the eigenvalues, i.e. $\lambda_i/\sum\lambda$.

```{r}
pca$sdev^2/sum(pca$sdev^2)
eigen(C)$values/sum(eigen(C)$values)
```

The small difference is again due to how the principal components are calculated, but biologically the interpretation is the same.

To understand how each principal component is constructed, we look at the loadings of each original variable onto the PCs.

```{r}
pca$loadings[1:3, 1:3]
```
Here, the first PC represents variation in `z2` and, to a lesser extent, `z3` (the two variables must thus be positively correlated). The second PC represents mostly `z1`, and also separates variation in `z2` from variation in `z3`.

A PCA can also be illustrated by a biplot.

```{r}
biplot(pca, col=c("grey", "black"), cex=c(.5, 1))
```

## Data exercise: principal component analysis

Choose any dataset comprosing three or more continuous variables (for example the *blossoms* data or the *alpine plants* data). Perform a principal component analysis and produce a biplot. Interpret the results in terms of which biological entitites are represented by each principal component.


## Principal component regression

One application of principal component analysis is principal component regression, where we fit the regression model to the principal components instead of the original variables. This could be useful if we have multiple correlated variables and want to include them while avoiding multicollinearity issues.

Below we first define a response variable $y$ based on the data we simulated from the multivariate normal distribution, and then fit a standard linear regression model.

```{r}
XX = as.data.frame(scale(X))
y = 0.5*XX$z1 -0.3*XX$z2 + 0.2*XX$z3 + rnorm(200, 0, 1)

m0 = lm(y~XX$z1+XX$z2+XX$z3)
summary(m0)
```

We then perform a principal component analysis, extract the three principal components, and fit a second linear models with the PCs as predictors.

```{r, fig.height=4, fig.width=4}
pca = princomp(XX)
biplot(pca, col=c("grey", "black"), cex=c(.5, 1))
```

```{r}
pc1 = pca$scores[,1]
pc2 = pca$scores[,2]
pc3 = pca$scores[,3]

m3 = lm(y~pc1+pc2+pc3)
summary(m3)
```

Notice that the $r^2$ of this model is the same as for the standard model fitted to the original variables. This is expected because we have just reorganized the original variables into new variables. In fact, we can calculate the slopes of the standard model from the slopes of the principal-component regression by using the loadings of the principal components,

$\beta=\mathbf{Q}\mathbf{b}$

where $\mathbf{Q}$ is a matrix of loadings for each PC (similar to eigenvectors), and $\mathbf{b}$ is a vector of regression coefficients.

```{r}
Q = pca$loadings
b = as.matrix(summary(m3)$coefficients[-1,1])
dim(Q)
dim(b)
Q %*% b
```

These are the slopes of the standard model, so we have not necessarily learned very much new by fitting the principal component regression. But what if we fit the model to only the first principal component?

```{r}
m1 = lm(y~pc1)
summary(m1)
```

The model explains less of the variance, but we have all the original variables represented. To understand how this translates into effects for the original variables, we have to also consider the loadings of each original variable onto the principal components.

```{r}
pca$loadings[1:3, 1:3]
```

## Data exercise: Principal component regression

Load the alpine plants dataset, and use principal component analysis to reduce the environmental variables into a reduced set of principal components (you can use a single PCA or several, for example one for temperature separately). Fit a principal-component regression and interpret the results.