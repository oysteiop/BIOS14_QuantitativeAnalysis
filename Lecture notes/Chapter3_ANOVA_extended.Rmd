---
title: "Processing and Analysis of Biological Data"
subtitle: "Chapter 3. The Linear Model II: Analysis of Variance"
author: "Ã˜ystein H. Opedal, Department of Biology, Lund University"
date: "`r Sys.Date()`"
output: pdf_document
fig_caption: yes
pandoc_args: ["--wrap=none"]
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Analysis of variance (ANOVA)
When our predictor variables are categorical (factors), linear models are used to perform analyses of variance. The parameter estimation works in much the same way as in regression, except that instead of estimating regression slopes, we are estimating group effects.

The aim of our ANOVA analysis is to partition the variance in our response variable into a set of additive components (recall, variances are additive). Based on this, we can evaluate whether the variance among groups is greater than the variance within groups, or more so than expected by chance.

The ANOVA framework is based on calculating "sums of squares" (SS), the sum of the squared deviations of each group mean from the grand mean (the variance among groups), and of each datapoint from either the grand mean (which is equal to the total variance in the data), or the group means (the residual or within-group variance).

```{r, fig.height=4, fig.width=8, fig.cap="Illustration of total, among-groups, and within-groups sums of squares. The total sum of squares is the total variance in the data, which can be partitioned into among-group and within-group (residual) components", echo=F}
set.seed(29)
groups = as.factor(rep(c("a", "b", "c"), each=5))
y = c(rnorm(5, 10, 3), rnorm(5, 13, 3), rnorm(5, 14, 3))
means = tapply(y, groups, mean)
xvals = as.numeric(groups)+rep(seq(-.15, .15, length.out=5), 3)

par(mfrow=c(1,2), mar=c(1,1,2,1))
plot(xvals, y, las=1,
     xlim=c(0.5, 3.5),
     xaxt="n", xlab="",
     yaxt="n", ylab="",
     main="Total and among-groups SS")
abline(h=mean(y), lty=1)
segments(c(.8, 1.8, 2.8), means, c(1.2, 2.2, 3.2), lwd=2)
segments(1:3, mean(y), 1:3, means, lwd=2)
segments(xvals, mean(y), xvals, y, lty=2)

plot(xvals, y, las=1,
     xlim=c(0.5, 3.5),
     xaxt="n", xlab="",
     yaxt="n", ylab="",
     main="Within-groups SS")
segments(c(.8, 1.8, 2.8), means, c(1.2, 2.2, 3.2), lwd=2)
segments(xvals, rep(means, each=5), xvals, y, lty=2)
```

To understand how the ANOVA analysis works, let's simulate some data, fit a linear model, and perform an ANOVA.

```{r, fig.width=4, fig.height=4}
set.seed(100)
groups = as.factor(rep(c("Low", "Medium", "High"), each=50))
x = c(rnorm(50, 10, 3), rnorm(50, 13, 3), rnorm(50, 14, 3))

plot(groups, x, las=1, xlab="", ylab="Body size (g)")
```

Plots like this are called "boxplots", and can be useful for visualising the distribution of data across factor levels or other groups. With the default settings, the boxes range from the 1st to the 3rd quartile, i.e. they span 50% of the data. The thick lines show the median, the "whiskers" extend to 1.5 times the inter-quantile range, and individual circles show outliers. This representation allows us to assess whether the data are roughly normally distributed within each group, which will tend to yield normally distributed residuals within each group, an assumption of the ANOVA model.

Boxplots are not directly useful for discussing ANOVA and variance partitioning though, and plots that show all the data can be more informative. Here is a rather elaborate plot combining a scatterplot (with values slightly "jittered" along the x-axis for clarity) and a boxplot.

EXTRA EXERCISE: Reproduce a plot similar to this for the ANOVA exercise.

```{r, fig.width=4, fig.height=4, echo=F}
plot(as.numeric(groups) + rnorm(150,0,0.03), x, 
     las=1, xlab="", ylab="Body size (g)",
     type="p", col="grey",
     xlim=c(0.5, 3.75), xaxt="n", yaxt="n")
axis(1, 1:3, labels=levels(groups))

means = tapply(x, groups, mean)
points(1:3, means, pch=16, col="black")

par(new=T)
plot(groups, x, at=c(1.3, 2.3, 3.3), boxwex=0.3, 
     xlim=c(0.5, 3.75), xaxt="n",
     las=1, xlab="", ylab="")
```


Just as for linear regression, we fit the model with the `lm` function and save the results to an object (here `m`). To perform an ANOVA based on the fitted model, we call the `anova` function.

```{r}
m = lm(x~groups)
anova(m)
```

The ANOVA table contains a lot of information. First, we learn about the number of degrees of freedom for each variable. For the `groups` variable (our focal factor), the 2 degrees of freedom is the number of groups in our data (3) - 1. The minus 1 comes from the fact that we had to estimate the mean in the data to obtain our sums of squares (the sum of the square deviations of data points from their group means). Similarly for residual degrees of freedom, we have 150 - 2 - 1, where the 2 comes from estimating the two contrasts (difference of group 2 and 3 from group 1), and the 1 is still the estimated mean.

The `Sum Sq` are the sums of squares, i.e. the sum of the squared deviations of each observation from the grand mean. The total sum of squares ($SS_T$) divided by $n-1$ gives the total variance of the sample.

```{r}
SS_T = 319.97+1200.43
SS_T/(150-1)
```

```{r}
var(x)
```

We can easily get the proportion of variance explained by the `groups` variable, which is the same as the $r^2$ for the model.

```{r}
319.97/SS_T
```

The `Mean Sq` is the variance attributable to each variable, conventionally called the mean sum of squares (the sum of squares divided by the degrees of freedom). The F-ratio (the test statistic in an ANOVA) is computed as the mean sum of squares for the focal variable divided by the mean residual sum of squares. Thus, it represents the ratio of the among-group variance to the within-group variance, but also the sample size which gives the residual degrees of freedom and thus, all else being equal, larger sample gives lower mean sum of squares and thus higher F-ratios and lower $P$-values.

In an ANOVA, a statistically supported among-group variance component such as the one above indicates that at least one group mean is different from the others. To further assess which groups are different, we can extract the typical summary table of the linear model.

```{r}
summary(m)
```

This contains some of the same information as the ANOVA table, but we now also obtain parameter estimates. The first parameter, the intercept, corresponds to the estimated mean for the first level of the `groups` factor. In this example this happens to be 'High', because H comes before L and M in the alphabet. The next two estimates represents *contrasts* from the reference group, and the associated hypothesis tests test the null hypothesis that the group has the same mean as the reference group.

The summary table also gives us directly the $r^2$, which is a simple ANOVA is defined as $1-SS_E/SS_T$, where $SS_E$ is the sum of squares for the error term (residuals), and $SS_T$ is the total sum of squares. (Control question: why could we do it even more simply above?).

The parameter estimates allow us to quantify the effect size, i.e. the magnitude of the difference between the groups. A useful way to report such differences is to compute the % difference (the contrast divided by the mean of the reference group, here -3.456/13.701 = -0.252), so that we can say that "Individuals in the low-food treatment were 25.2% smaller than individuals in the high-food treatment".

Note that if we want a different reference group, we can change the order of the factor levels.

```{r}
groups = factor(groups, levels=c("Low", "Medium", "High"))
m = lm(x~groups)
summary(m)
```

Sometimes we also want to suppress the intercept of the model, and thus estimate the mean and standard error for each level of the predictor. We can do this by adding `-1` to the model formula (what comes after the `~` sign). This could be useful for example, if we wanted to obtain the estimated mean for each group, associated for example with a 95% confidence interval.

```{r}
m = lm(x~groups-1)
summary(m)$coef
confint(m)
```

## Two-way ANOVA
Analyses of variance can also be performed with more than one factor variable. If we have two factors, we can talk about two-way ANOVA, and so on. A typical example from biology is when we have performed a factorial experiment, and want to assess the effects of each experimental factor and their potential interaction.

With two factors, a full model can be formulated as `y ~ factor1 * factor2`. Recall that in `R` syntax, the * means both main effects and their interaction, while a : means only the interaction. A detectable interaction term in this model would indicate that the effect of factor 1 depends on the level of factor 2 (and *vice versa*). If we are analysing an experiment where we have manipulated both temperature and nitrogen supply, an interaction would mean that the effect of temperature depend on the nitrogen level.

### Data exercise: analysing a factorial experiment
The following data are from an experiment where female butterflies reared on two different host plants were allowed to oviposit on the same two host plants. The data include developmental time for the larvae, the adult weight, and the growth rate of the larvae.

Analyse the data to assess effects of maternal vs. larval host plant on one or more response variable. Interpret the results and produce a nice plot to illustrate.

```{r}
dat = read.csv("datasets/butterflies.csv")
names(dat)
```

As a first step, let us compute some summary statistics, like the mean development time for each combination of larval and maternal host plant.

```{r}
dat$MaternalHost = paste0(dat$MaternalHost, "M")
dat$LarvalHost = paste0(dat$LarvalHost, "L")
means = tapply(dat$DevelopmentTime, list(dat$MaternalHost, dat$LarvalHost), mean)
means
```

```{r, fig.height=4, fig.width=4, echo=F, fig.cap="Larval developmental time depending on larval and maternal host plant" }
ses = tapply(dat$DevelopmentTime, 
       list(dat$MaternalHost, dat$LarvalHost), 
       function(x) sd(x)/sqrt(sum(!is.na(x))))
ses

plot(c(0.97, 1.03), means[,1], ylim=c(18, 40), xlim=c(0.8, 2.2),
     xlab="Larval host", 
     ylab="Developmental time (days)",
     xaxt="n", las=1, pch=c(21,16), col="white")
axis(1, 1:2, labels=c(expression(paste(italic(Barbarea))),
                      expression(paste(italic(Berteroa)))))

arrows(c(0.97,1.03), means[,1]-ses[,1], c(0.97,1.03), 
       means[,1]+ses[,1], length=0.05, angle=90, code=3)
arrows(c(1.97,2.03), means[,2]-ses[,2], c(1.97,2.03), 
       means[,2]+ses[,2], length=0.05, angle=90, code=3)

segments(0.97, means[1,1], 1.97, means[1,2], lty=2)
segments(1.03, means[2,1], 2.03, means[2,2])

points(c(0.97, 1.03), means[,1], pch=c(21,16), bg="white")
points(c(1.97, 2.03), means[,2], pch=c(21, 16), bg="white")

legend("topleft", c("Maternal host", 
                    expression(paste(italic(Barbarea))),
                    expression(paste(italic(Berteroa)))), 
       bty="n", pch=c(NA,21,16))
```

To get started with the formal analyses, fit a linear model with larval and maternal host, as well as their interaction, as predictors. Produce an ANOVA table.

```{r}
names(dat)
m = lm(DevelopmentTime~MaternalHost*LarvalHost, data=dat)
anova(m)
```

To interpret the results, we look first at the ANOVA table. There are detectable effects of both larval and maternal host plants, with more variance explained by larval host than maternal host (based on the larger sum of squares for larval host). Furthermore, the effects of larval and maternal host are not independent, as indicated by a detectable interaction. The variance explained by the interaction term is limited though, as we can also see from the graph.

\newpage

```{r}
summary(m)
```
To interpret the summary table from a two-way ANOVA, we need to know how to read the parameter estimates. The Intercept is, as before, the estimated mean for the reference group. Here, the reference group is the combination of the the first level of each factor, in this case *Barbarea* as both maternal and larval host plant. The following to terms are the contrasts between the reference group and the alternative maternal and larval host plants, respectively. Finally, the interaction effect (the last term above), gives the *added* effect of changing both the maternal and the larval host plant. Hence, to obtain the estimated mean for the combination maternal host *Berteroa* and larval host *Berteroa*, we sum all the parameter estimates. Check that this is indeed the case!

To quantify the effect size, it is useful to first compute the mean development time for each larval and maternal host plant.

```{r, echo=F}
colMeans(means)
rowMeans(means)
```

Based on this, we could write the methods and results like this:

*Methods*
To assess differences in development time between larvae grown on *Barbarea* and *Berteroa*, and between larvae whose mothers were grown on the same two hosts, we fitted a linear model with development time as response variable, and larval and maternal hosts as predictors, and performed an analysis of variance based on the fitted model. To assess transgenerational effects, we also includes the interaction term between maternal and larval host. Thus, in R syntax, our model took the form

$$Development Time \sim Larval Host * Maternal Host$$

*Results*
The larvae developed 22.1% faster when grown on Barbarea than when grown on Berteroa (mean development time = 22.6 and 29.0 days, respectively, $F_{1,283}=765.21$, Fig. 1). Larvae whose mothers were grown on Barbarea developed 10.7% faster (mean development time = 24.3 and 27.3 days, respectively, $F_{1,283}=177.90$). The difference in development time between larval host plants was slightly larger when the mother was grown on Berteroa than when the mother was grown on Barbarea (24.2% vs. 19.6% reduction in developmental time on Barbarea, respectively).

